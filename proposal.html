<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>AirScribe: Writing on a Virtual Canvas using Finger Tracking</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="56a6ff49-174e-45d4-8f9c-bc277554a7d1" class="page sans"><header><img class="page-cover-image" src="https://images.unsplash.com/photo-1598520106830-8c45c2035460?ixlib=rb-1.2.1&amp;q=85&amp;fm=jpg&amp;crop=entropy&amp;cs=srgb&amp;ixid=eyJhcHBfaWQiOjYzOTIxfQ" style="object-position:center 50.09%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">🖊️</span></div><h1 class="page-title">AirScribe: Writing on a Virtual Canvas using Finger Tracking</h1></header><div class="page-body"><p id="eff79e86-ffa4-49df-93f9-5985d79ba607" class="">Kimaya Colaço - <a href="mailto:kimayacolaco@gatech.edu">kimayacolaco@gatech.edu</a></p><p id="a3c7de8a-6934-4d37-8e53-a000e96b680f" class="">Parth Nagpal - <a href="mailto:parthnagpal@gatech.edu">parthnagpal@gatech.edu</a></p><p id="078bd22e-7632-4e46-97a8-865627d97507" class="">Saumya Jain - <a href="mailto:samjain0913@gatech.edu">samjain0913@gatech.edu</a></p><p id="01f45fe1-e1cf-4923-a56c-a019d3d9cedb" class="">Gursimran Singh - <a href="mailto:gsingh@gatech.edu">gsingh@gatech.edu</a></p><h1 id="569eb407-8645-4165-822a-02e780c3a87c" class="">Problem Statement</h1><p id="772be768-8fcf-422a-a7b7-aecfabdc5149" class="">AirScribe enables users to draw or write using gestures in the air and capture this on a virtual canvas. </p><p id="df9b7248-1243-4bdf-88ac-d294844d7d52" class="">We see tremendous potential as it has several applications in the real world. With professors having to teach virtual classes, it is difficult to share notes or write out concepts like one would on a typical whiteboard. With AirScribe, professors will be able to teach as if they had a whiteboard in front of them. They can write or draw out concepts, as they would on a normal whiteboard, in the air in front of them. </p><p id="05369be8-f069-403e-8e3e-7c7eaa4ebf61" class="">The input would be the finger gestures and the output would be the writing on the virtual canvas. These gestures will be recognized, read in and correctly inverted by our system. The system will then display the captured output on a virtual canvas with a plain background. AirScribe will also have additional tools and features to make professors&#x27; task easier including different colors, widths, eraser, clear canvas etc. </p><p id="37626ec4-ece8-4361-b303-2e76277a7f98" class="">
</p><figure id="c7c5df68-4e2a-4cc8-a1e0-2523407fe467" class="image"><a href="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/virt.png"><img style="width:600px" src="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/virt.png"/></a></figure><h1 id="880042b5-6c3e-4d68-973c-3f43d2d1eedc" class="">Approach</h1><p id="8e8e3d2b-28a8-44e5-978b-a604b8fe2fed" class="">We plan to implement AirScribe using advanced computer vision and machine learning methods. This includes the following steps:</p><ol id="d6158574-5110-463c-834e-6e9b894992e8" class="numbered-list" start="1"><li><strong>Preprocessing Video Feed:</strong> We extract skin color pixels of the user and that region is selected. This is then converted to a grayscale image and thresholding is applied.</li></ol><ol id="136ec5f8-bf06-47c2-98b5-d27cd7581739" class="numbered-list" start="2"><li><strong>Gaussian Blurring:</strong> The video frames from the webcam will have noise, especially in lower quality image feeds. Using a Gaussian Blur will reduce sensor noise and effects of low lighting. We can blur the image by convolving it with a low-pass filter kernel. The kernel will be a Gaussian matrix. The output image is effectively blurred.</li></ol><ol id="f5a4247f-03fc-40c2-9b77-4f2c0db58f2a" class="numbered-list" start="3"><li><strong>Convex Hull Detection:</strong> We draw a polygon around the hand to find the contours of the hand. The convex hull algorithm uses computations where a non-ambiguous and efficient representation of the required convex shape is constructed.</li></ol><ol id="4d1d4539-318c-4404-84cd-ef33b8ad4a07" class="numbered-list" start="4"><li><strong>Fingertip Detection:</strong> From the obtained region, we calculate the contours of the hand. The contour points are saved and we detect the tip by finding the contour point with the maximum y-coordinate.</li></ol><ol id="b07d45f0-ccb1-4802-90f9-a328b8496a05" class="numbered-list" start="5"><li><strong>Tracking the Fingertip:</strong> Once our fingertip has been determine, we need to plot the coordinates. When the user draws a pattern with that finger, the positions and points of the finger get stored in a linked list. The list grows in size as new points are appended to it, while the finger moves. From this, we map the linked list on a smaller bitmap.</li></ol><ol id="0d2f0789-b42f-4330-8849-cf145ad559d7" class="numbered-list" start="6"><li><strong>Recognition of Character: </strong>Using our created bitmap, we pass this into our model and recognize the character drawn. This uses a convolutional neural network.<ol id="07a5a78b-91db-43db-9bdc-6ab8c3888447" class="numbered-list" start="1"><li>This project uses a dataset where the training is done using a set of handwritten digits of the NIST database converted to a 28 x 28 pixel image format. This dataset can be used for classes of [0-9], [a-z] and [A-Z].</li></ol></li></ol><p id="d6015c7c-441f-4290-aa0b-78efef45d3b0" class="">
</p><figure id="63b4d563-f06e-4828-88ad-35dfcaed1404" class="image"><a href="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/Screen_Shot_2020-10-01_at_6.56.27_PM.png"><img style="width:1920px" src="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/Screen_Shot_2020-10-01_at_6.56.27_PM.png"/></a><figcaption>Image from &quot;Visual Gesture Recognition for Text Writing in Air&quot;</figcaption></figure><h1 id="03f18b98-0150-4866-82a7-cd4bb99b4493" class="">Experiments and Results</h1><p id="cfdc936a-b1d6-4f03-970b-ba042538a645" class=""><strong>Experimental Setup</strong></p><p id="44fb8891-cfdb-45d7-bd5d-30f5f05b4b68" class="">We will test our program for accurate finger tracking and correct character recognition</p><ul id="1ecfad2d-bfcd-4df1-9915-ff07fdebb5b8" class="bulleted-list"><li><strong>Finger Tracking</strong>: Our computer vision algorithm will generate a 28x28 image of the character being drawn in air using the finger. We can manually test this by drawing a character in front of the camera and then comparing it with the generated image. </li></ul><ul id="59ef746e-9335-4ed6-96e8-a4779414191e" class="bulleted-list"><li><strong>Convolutional Neural Network: </strong>Our CNN, during the training and testing phase, will generate an accuracy metric for each character. This can be further tested with our own data. </li></ul><p id="1200f242-aec8-4930-a9f6-5ed33e7c396f" class="">We hope to use these experiments to test our system&#x27;s accuracy and improve upon it. </p><p id="92bad4b9-a3aa-46cc-9b70-a7d4d43db075" class="">
</p><p id="5d7ed22e-f441-4f9c-b574-9f5ba54b64c4" class=""><strong>Datasets</strong></p><ul id="5573caa5-818c-475a-8287-549b5af5a381" class="bulleted-list"><li>We have planned to use the EMNIST dataset of 28x28 grayscale images of handrwitten characters (letters and digits) available for use <a href="https://www.tensorflow.org/datasets/catalog/emnist">here</a>. We will use this dataset to train our CNN. The split has 697,932 images for training and 116,323 images for testing. </li></ul><ul id="b663cf1f-97e7-4106-9fac-c83ad7b11954" class="bulleted-list"><li>We will also generate our own dataset of handwritten/finger-tracked images for testing the neural network. (This will be generated by the group members)</li></ul><p id="82b6a868-6960-4835-afc2-5fbf6a36928f" class="">
</p><p id="ed3f961b-ec82-4c54-9ebb-7347c5e5a7c1" class=""><strong>What will we be implementing ourselves?</strong></p><ul id="fe616ea7-5ec9-4e13-a7f9-f9cbd7ef5cc0" class="bulleted-list"><li>We read multiple papers on how to implement finger-tracking but we will write our own code often taking help from the class material on video processing and online resources/blogs. We will cite anything we read or use. </li></ul><ul id="ed802132-d132-4ebc-8ef8-7e52cfbae922" class="bulleted-list"><li>We have found multiple blogs on how to implement a CNN for EMNIST database using <a href="https://www.kaggle.com/pagedavid/cnn-on-tensorflow">Tensorflow</a> or <a href="https://www.kaggle.com/ashwani07/emnist-using-keras-cnn">Keras</a>. To build over this, we are planning to implement our own continuous stroke recogniton (cursive handwriting) over a letter-by-letter approach. </li></ul><ul id="c54a07df-0495-4afd-8c13-e9cc29247a5d" class="bulleted-list"><li>For making the program more useful, we plan to introdouce different modes activated by different gestures: Straight line mode(for underlining), highlighting mode, shape(for making boxes and other shapes) mode etc. </li></ul><p id="407ae4cd-18ae-4b8b-9c7c-a438070842a0" class="">
</p><p id="e24b07e5-5bf9-4b6b-a3f4-e3f9bb2cce96" class=""><strong>What is the definition of success with respect to our project?</strong></p><ul id="485c96a1-cc7a-4025-be88-97dcb5384335" class="bulleted-list"><li>We would consider our project successful if we are able to build a working pipeline for the processes we have mentioned above. </li></ul><ul id="e32ae85c-c084-4ada-b2c7-149b821550b5" class="bulleted-list"><li>In terms of the product, we aim to build a solution that can help instructors seamlessly write notes in the air just using their fingers, providing them with an advanced interface to interact with their students in situations of remote learning. </li></ul><ul id="e6308a3d-a415-4e24-a29e-cd41e7e6e970" class="bulleted-list"><li>Our north star metric would be a deployable platform!</li></ul><p id="11f3577f-38d1-415a-a0a5-45bba7f01777" class="">
</p><figure id="0f485d56-0877-49a0-b256-13bd569fb25f" class="image"><a href="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/emnist.png"><img style="width:850px" src="AirScribe%20Writing%20on%20a%20Virtual%20Canvas%20using%20Finger%20c7c5df684e2a4cc8a1e02523407fe467/emnist.png"/></a><figcaption>EMNIST Dataset</figcaption></figure><p id="967b049d-ed0c-4525-ab37-109b67e04a5a" class="">
</p><div id="9e682273-b2c4-4bae-a018-de942a9880fe" class="collection-content"><h4 class="collection-title">References</h4><table class="collection-content"><thead><tr><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesTitle"><path d="M7.73943662,8.6971831 C7.77640845,8.7834507 7.81338028,8.8943662 7.81338028,9.00528169 C7.81338028,9.49823944 7.40669014,9.89260563 6.91373239,9.89260563 C6.53169014,9.89260563 6.19894366,9.64612676 6.08802817,9.30105634 L5.75528169,8.33978873 L2.05809859,8.33978873 L1.72535211,9.30105634 C1.61443662,9.64612676 1.2693662,9.89260563 0.887323944,9.89260563 C0.394366197,9.89260563 0,9.49823944 0,9.00528169 C0,8.8943662 0.0246478873,8.7834507 0.0616197183,8.6971831 L2.46478873,2.48591549 C2.68661972,1.90669014 3.24119718,1.5 3.90669014,1.5 C4.55985915,1.5 5.12676056,1.90669014 5.34859155,2.48591549 L7.73943662,8.6971831 Z M2.60035211,6.82394366 L5.21302817,6.82394366 L3.90669014,3.10211268 L2.60035211,6.82394366 Z M11.3996479,3.70598592 C12.7552817,3.70598592 14,4.24823944 14,5.96126761 L14,9.07922535 C14,9.52288732 13.6549296,9.89260563 13.2112676,9.89260563 C12.8169014,9.89260563 12.471831,9.59683099 12.4225352,9.19014085 C12.028169,9.6584507 11.3257042,9.95422535 10.5492958,9.95422535 C9.60035211,9.95422535 8.47887324,9.31338028 8.47887324,7.98239437 C8.47887324,6.58978873 9.60035211,6.08450704 10.5492958,6.08450704 C11.3380282,6.08450704 12.040493,6.33098592 12.4348592,6.81161972 L12.4348592,5.98591549 C12.4348592,5.38204225 11.9172535,4.98767606 11.1285211,4.98767606 C10.6602113,4.98767606 10.2411972,5.11091549 9.80985915,5.38204225 C9.72359155,5.43133803 9.61267606,5.46830986 9.50176056,5.46830986 C9.18133803,5.46830986 8.91021127,5.1971831 8.91021127,4.86443662 C8.91021127,4.64260563 9.0334507,4.44542254 9.19366197,4.34683099 C9.87147887,3.90316901 10.6232394,3.70598592 11.3996479,3.70598592 Z M11.1778169,8.8943662 C11.6830986,8.8943662 12.1760563,8.72183099 12.4348592,8.37676056 L12.4348592,7.63732394 C12.1760563,7.29225352 11.6830986,7.11971831 11.1778169,7.11971831 C10.5616197,7.11971831 10.056338,7.45246479 10.056338,8.0193662 C10.056338,8.57394366 10.5616197,8.8943662 11.1778169,8.8943662 Z M0.65625,11.125 L13.34375,11.125 C13.7061869,11.125 14,11.4188131 14,11.78125 C14,12.1436869 13.7061869,12.4375 13.34375,12.4375 L0.65625,12.4375 C0.293813133,12.4375 4.43857149e-17,12.1436869 0,11.78125 C-4.43857149e-17,11.4188131 0.293813133,11.125 0.65625,11.125 Z"></path></svg></span>Title</th><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesText"><path d="M7,4.56818 C7,4.29204 6.77614,4.06818 6.5,4.06818 L0.5,4.06818 C0.223858,4.06818 0,4.29204 0,4.56818 L0,5.61364 C0,5.88978 0.223858,6.11364 0.5,6.11364 L6.5,6.11364 C6.77614,6.11364 7,5.88978 7,5.61364 L7,4.56818 Z M0.5,1 C0.223858,1 0,1.223858 0,1.5 L0,2.54545 C0,2.8216 0.223858,3.04545 0.5,3.04545 L12.5,3.04545 C12.7761,3.04545 13,2.8216 13,2.54545 L13,1.5 C13,1.223858 12.7761,1 12.5,1 L0.5,1 Z M0,8.68182 C0,8.95796 0.223858,9.18182 0.5,9.18182 L11.5,9.18182 C11.7761,9.18182 12,8.95796 12,8.68182 L12,7.63636 C12,7.36022 11.7761,7.13636 11.5,7.13636 L0.5,7.13636 C0.223858,7.13636 0,7.36022 0,7.63636 L0,8.68182 Z M0,11.75 C0,12.0261 0.223858,12.25 0.5,12.25 L9.5,12.25 C9.77614,12.25 10,12.0261 10,11.75 L10,10.70455 C10,10.4284 9.77614,10.20455 9.5,10.20455 L0.5,10.20455 C0.223858,10.20455 0,10.4284 0,10.70455 L0,11.75 Z"></path></svg></span>Citation</th></tr></thead><tbody><tr id="24644e04-f2fa-456c-87f4-c424db89fc9f"><td class="cell-title"><a href="https://www.notion.so/Hand-Gesture-Recognition-with-Convolution-Neural-Networks-24644e04f2fa456c87f4c424db89fc9f">Hand Gesture Recognition with Convolution Neural Networks</a></td><td class="cell-n&gt;J|">F. Zhan, &quot;Hand Gesture Recognition with Convolution Neural Networks,&quot; 2019 IEEE 20th International   Conference on Information Reuse and Integration for Data Science (IRI), Los Angeles, CA, USA, 2019, pp. 295-298, doi: 10.1109/IRI.2019.00054.</td></tr><tr id="2027a9f6-0159-4b4c-866a-197421e75a61"><td class="cell-title"><a href="https://www.notion.so/Build-a-Handwritten-Text-Recognition-System-using-TensorFlow-2027a9f601594b4c866a197421e75a61">Build a Handwritten Text Recognition System using TensorFlow</a></td><td class="cell-n&gt;J|">Scheidl, H. (2020, August 09). Build a Handwritten Text Recognition System using TensorFlow. Retrieved October 01, 2020, from https://towardsdatascience.com/build-a-handwritten-text-recognition-system-using-tensorflow-2326a3487cd5</td></tr><tr id="fa765c94-3a51-4e0b-988e-da51f724f983"><td class="cell-title"><a href="https://www.notion.so/Visual-Gesture-Recognition-for-Text-Writing-in-Air-fa765c943a514e0b988eda51f724f983">Visual Gesture Recognition for Text Writing in Air</a></td><td class="cell-n&gt;J|">V. Joseph, A. Talpade, N. Suvarna and Z. Mendonca, &quot;Visual Gesture Recognition for Text Writing in Air,&quot; 2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS), Madurai, India, 2018, pp. 23-26, doi: 10.1109/ICCONS.2018.8663176.</td></tr></tbody></table></div><p id="f01b5a4a-f637-4c95-b1bf-6af7f9b42e7c" class="">
</p></div></article></body></html>